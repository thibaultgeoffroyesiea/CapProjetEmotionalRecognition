{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train head:  ['Image/neutral/01_038.jpg', 'Image/angry/05_077.jpg', 'Image/happy/02_167.jpg', 'Image/happy/02_054.jpg', 'Image/angry/05_036.jpg'] \n",
      " [0.0, 4.0, 1.0, 1.0, 4.0]\n",
      "Test head:  ['Image/happy/02_195.jpg', 'Image/happy/02_067.jpg', 'Image/disgust/07_184.jpg', 'Image/neutral/01_017.jpg', 'Image/fear/04_204.jpg'] \n",
      " [1.0, 1.0, 6.0, 0.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18,googlenet, densenet121,mobilenet_v3_small,mobilenet_v3_large\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "def multiple_dir_files_to_df(dirs, label):\n",
    "    df = pd.DataFrame()\n",
    "    for idx,dir in enumerate(dirs):\n",
    "        files = os.listdir(dir)\n",
    "        files = [dir + '/' + file for file in files]\n",
    "        df = pd.concat([df,pd.DataFrame({'file': files, 'label': label[idx]})])\n",
    "    return df\n",
    "\n",
    "common_path=\"Image/\"\n",
    "\n",
    "labels=[\"neutral\",\"happy\",\"sad\",\"fear\",\"angry\",\"surprise\",\"disgust\"]\n",
    "\n",
    "database=[common_path+label for label in labels]\n",
    "\n",
    "def training_test_model(train_loader,test_loader,model,model_name,epochs=50, learning_rate=0.001,weight_decay=1e-4,gamma=0.9,early_stopping_patience=5,early_stopping_save=True):\n",
    "    # Initialiser le modèle et le journal\n",
    "    model = model\n",
    "    patience = early_stopping_patience\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    accuracy_test_list=[]\n",
    "    accuracy_train_list=[]\n",
    "    loss_test_list=[]\n",
    "    loss_train_list=[]\n",
    "    \n",
    "    # Entraîner le modèle\n",
    "    num_epochs = epochs\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Définir l'optimiseur et la fonction de perte\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer,gamma=gamma)\n",
    "    # Entraîner le modèle\n",
    "    for epoch in range(num_epochs):\n",
    "        accuracy_train=[]\n",
    "        loss_train=[]\n",
    "        accuracy_test=[]\n",
    "        loss_test=[]\n",
    "        current_lr = [param_group['lr'] for param_group in optimizer.param_groups][0]\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            # Get data to cuda if possible\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            # forward\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets.long())\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            likelihood, predictions = scores.max(1)\n",
    "            loss_train.append(loss.item())\n",
    "            accuracy_train.append((predictions == targets).float().mean().item())\n",
    "\n",
    "            # gradient descent or adam step\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        for batch_idx, (data, targets) in enumerate(test_loader):\n",
    "            # Get data to cuda if possible\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            # forward\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets.long())\n",
    "            \n",
    "            likelihood, predictions = scores.max(1)\n",
    "            loss_test.append(loss.item())\n",
    "            accuracy_test.append((predictions == targets).float().mean().item())\n",
    "\n",
    "        accuracy_train=np.array(accuracy_train).mean()\n",
    "        loss_train=np.array(loss_train).mean()\n",
    "        accuracy_test=np.array(accuracy_test).mean()\n",
    "        loss_test=np.array(loss_test).mean()\n",
    "        accuracy_test_list.append(accuracy_test)\n",
    "        accuracy_train_list.append(accuracy_train)\n",
    "        loss_test_list.append(loss_test)\n",
    "        loss_train_list.append(loss_train)\n",
    "        \n",
    "        if early_stopping_save:\n",
    "            if loss_test < best_val_loss:\n",
    "                best_val_loss = loss_test\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            if counter > patience:\n",
    "                print(\"Early stopping at epoch: \",epoch)\n",
    "                print(\"Saving model...\")\n",
    "                torch.save(model.state_dict(), f\"./models_tg/{model_name+'_'+str(epoch)}.pth\")\n",
    "                return accuracy_test_list,accuracy_train_list,loss_test_list,loss_train_list\n",
    "        print(f\"Train epoch: {epoch}, accuracy = {accuracy_train} ,loss = {loss_train}, lr = {np.round(current_lr,6)}\")\n",
    "        print(f\"Test epoch: {epoch}, accuracy = {accuracy_test} ,loss = {loss_test}\")\n",
    "    torch.save(model.state_dict(), f\"./models/{model_name}.pth\")\n",
    "    return accuracy_test_list,accuracy_train_list,loss_test_list,loss_train_list\n",
    "\n",
    "def plot_train_loss(accuracy_test_list,accuracy_train_list,loss_test_list,loss_train_list,model_name):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(accuracy_test_list,label=\"accuracy_test\")\n",
    "    plt.plot(accuracy_train_list,label=\"accuracy_train\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy for \"+model_name+\" model\")\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(loss_test_list,label=\"loss_test\")\n",
    "    plt.plot(loss_train_list,label=\"loss_train\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss for \"+model_name+\" model\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "df = multiple_dir_files_to_df(database,np.linspace(0,6,7))\n",
    "X,y=df[\"file\"].tolist(),df[\"label\"].tolist()\n",
    "\n",
    "#Split the dataset into train and test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "print(\"Train head: \",X_train[:5],\"\\n\",y_train[:5])\n",
    "print(\"Test head: \",X_test[:5],\"\\n\",y_test[:5])\n",
    "\n",
    "# Définir un Dataset personnalisé\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_names, labels, transform=None):\n",
    "        self.file_names = file_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f'{self.file_names[idx]}' \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return image, label\n",
    "    \n",
    "# Define the image transformations\n",
    "transform = transforms.Compose([\n",
    "    # transforms.CenterCrop((500, 500)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Créer les jeux de données et les chargeurs de données\n",
    "train_dataset = CustomDataset(X_train, y_train, transform)\n",
    "test_dataset = CustomDataset(X_test, y_test, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mobilenet = mobilenet_v3_small(weights='IMAGENET1K_V1')\n",
    "        self.mobilenet.classifier[3] = nn.Linear(1024, 7)\n",
    "    def forward(self, x):\n",
    "        return self.mobilenet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#check if gpu is used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MobileNet()\n",
    "accuracy_test_list,accuracy_train_list,loss_test_list,loss_train_list=training_test_model(train_loader,test_loader,model,\"MobileNet\",epochs=50, learning_rate=0.001,weight_decay=1e-4,gamma=0.9,early_stopping_patience=5,early_stopping_save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
